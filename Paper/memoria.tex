%----------
%   IMPORTANTE
%----------

% Esta plantilla está basada en las recomendaciones de la guía "Trabajo fin de Grado: Escribir el TFG", que encontrarás en http://uc3m.libguides.com/TFG/escribir
% contiene recomendaciones de la Biblioteca basadas principalmente en estilos APA e IEEE, pero debes seguir siempre las orientaciones de tu Tutor de TFG y la normativa de TFG para tu titulación.



% ESTA PLANTILLA ESTÁ BASADA EN EL ESTILO IEEE


%----------
%	CONFIGURACIÓN DEL DOCUMENTO
%----------

\documentclass[12pt]{report} % fuente a 12pt

% MÁRGENES: 2,5 cm sup. e inf.; 3 cm izdo. y dcho.
\usepackage[
a4paper,
vmargin=2.5cm,
hmargin=3cm
]{geometry}

% INTERLINEADO: Estrecho (6 ptos./interlineado 1,15) o Moderado (6 ptos./interlineado 1,5)
\renewcommand{\baselinestretch}{1.15}
\parskip=6pt

% DEFINICIÓN DE COLORES para portada y listados de código
\usepackage[table]{xcolor}
\definecolor{azulUC3M}{RGB}{0,0,102}
\definecolor{gray97}{gray}{.97}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}

% Soporte para GENERAR PDF/A --es importante de cara a su inclusión en e-Archivo porque es el formato óptimo de preservación y a la generación de metadatos, tal y como se describe en http://uc3m.libguides.com/ld.php?content_id=31389625. 

% En la plantilla incluimos el archivo OUTPUT.XMPDATA. Puedes descargar este archivo e incluir los metadatos que se incorporarán al archivo PDF cuando compiles el archivo memoria.tex. Después vuelve a subirlo a tu proyecto. 
\usepackage[a-1b]{pdfx}

% ENLACES
\usepackage{hyperref}
\hypersetup{colorlinks=true,
	linkcolor=black, % enlaces a partes del documento (p.e. índice) en color negro
	urlcolor=blue} % enlaces a recursos fuera del documento en azul

% EXPRESIONES MATEMÁTICAS
\usepackage{amsmath,amssymb,amsfonts,amsthm}

% Codificación caracteres
\usepackage{txfonts} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Definición idioma español
\usepackage[spanish, es-tabla]{babel} 
\usepackage[babel, spanish=spanish]{csquotes}
\AtBeginEnvironment{quote}{\small}

% diseño de PIE DE PÁGINA
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rfoot{\thepage}
\fancypagestyle{plain}{\pagestyle{fancy}}

% DISEÑO DE LOS TÍTULOS de las partes del trabajo (capítulos y epígrafes o subcapítulos)
\usepackage{titlesec}
\usepackage{titletoc}
\titleformat{\chapter}[block]
{\large\bfseries\filcenter}
{\thechapter.}
{5pt}
{\MakeUppercase}
{}
\titlespacing{\chapter}{0pt}{0pt}{*3}
\titlecontents{chapter}
[0pt]                                               
{}
{\contentsmargin{0pt}\thecontentslabel.\enspace\uppercase}
{\contentsmargin{0pt}\uppercase}                        
{\titlerule*[.7pc]{.}\contentspage}                 

\titleformat{\section}
{\bfseries}
{\thesection.}
{5pt}
{}
\titlecontents{section}
[5pt]                                               
{}
{\contentsmargin{0pt}\thecontentslabel.\enspace}
{\contentsmargin{0pt}}
{\titlerule*[.7pc]{.}\contentspage}

\titleformat{\subsection}
{\normalsize\bfseries}
{\thesubsection.}
{5pt}
{}
\titlecontents{subsection}
[10pt]                                               
{}
{\contentsmargin{0pt}                          
	\thecontentslabel.\enspace}
{\contentsmargin{0pt}}                        
{\titlerule*[.7pc]{.}\contentspage}  


% DISEÑO DE TABLAS
\usepackage{multirow} % permite combinar celdas 
\usepackage{caption} % para personalizar el título de tablas y figuras
\usepackage{floatrow} % utilizamos este paquete y sus macros \ttabbox y \ffigbox para alinear los nombres de tablas y figuras de acuerdo con el estilo definido.
\usepackage{array} % con este paquete podemos definir en la siguiente línea un nuevo tipo de columna para tablas: ancho personalizado y contenido centrado
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\DeclareCaptionFormat{upper}{#1#2\uppercase{#3}\par}

% Diseño de tabla para ingeniería
\captionsetup*[table]{
	format=upper,
	name=TABLA,
	justification=centering,
	labelsep=period,
	width=.75\linewidth,
	labelfont=small,
	font=small
}

% DISEÑO DE FIGURAS. 
\usepackage{graphicx}
\graphicspath{{images/}} % ruta a la carpeta de imágenes

% Diseño de figuras para ingeniería
\captionsetup[figure]{
	format=hang,
	name=Fig.,
	singlelinecheck=off,
	labelsep=period,
	labelfont=small,
	font=small		
}

% NOTAS A PIE DE PÁGINA
\usepackage{chngcntr} % para numeración continua de las notas al pie
\counterwithout{footnote}{chapter}

% LISTADOS DE CÓDIGO
% soporte y estilo para listados de código. Más información en https://es.wikibooks.org/wiki/Manual_de_LaTeX/Listados_de_código/Listados_con_listings
\usepackage{listings}

% definimos un estilo de listings
\lstdefinestyle{estilo}{ frame=Ltb,
	framerule=0pt,
	aboveskip=0.5cm,
	framextopmargin=3pt,
	framexbottommargin=3pt,
	framexleftmargin=0.4cm,
	framesep=0pt,
	rulesep=.4pt,
	backgroundcolor=\color{gray97},
	rulesepcolor=\color{black},
	%
	basicstyle=\ttfamily\footnotesize,
	keywordstyle=\bfseries,
	stringstyle=\ttfamily,
	showstringspaces = false,
	commentstyle=\color{gray45},     
	%
	numbers=left,
	numbersep=15pt,
	numberstyle=\tiny,
	numberfirstline = false,
	breaklines=true,
	xleftmargin=\parindent
}

\captionsetup*[lstlisting]{font=small, labelsep=period}
% fijamos el estilo a utilizar 
\lstset{style=estilo}
\renewcommand{\lstlistingname}{\uppercase{Código}}


%BIBLIOGRAFÍA 

% CONFIGURACIÓN PARA LA BIBLIOGRAFÍA IEEE
\usepackage[backend=biber, style=ieee, isbn=false,sortcites, maxbibnames=6, minbibnames=1]{biblatex} % Configuración para el estilo de citas de IEEE, recomendado para el área de ingeniería. "maxbibnames" indica que a partir de 6 autores trunque la lista en el primero (minbibnames) y añada "et al." tal y como se utiliza en el estilo IEEE.

% Añadimos las siguientes indicaciones para mejorar la adaptación del estilos en español
\DefineBibliographyStrings{spanish}{%
	andothers = {et\addabbrvspace al\adddot}
}
\DefineBibliographyStrings{spanish}{
	url = {\adddot\space[En línea]\adddot\space Disponible en:}
}
\DefineBibliographyStrings{spanish}{
	urlseen = {Acceso:}
}
\DefineBibliographyStrings{spanish}{
	pages = {pp\adddot},
	page = {p.\adddot}
}

\addbibresource{referencias.bib} % llama al archivo referencias.bib en el que deberá estar la bibliografía utilizada


%-------------
%	DOCUMENTO
%-------------

\begin{document}
\pagenumbering{roman} % Se utilizan cifras romanas en la numeración de las páginas previas al cuerpo del trabajo
	
%----------
%	PORTADA
%----------	
\begin{titlepage}
	\begin{sffamily}
	\color{azulUC3M}
	\begin{center}
		\begin{figure}[H] %incluimos el logotipo de la Universidad
			\makebox[\textwidth][c]{\includegraphics[width=16cm]{logo_UC3M.png}}
		\end{figure}
		\vspace{2.5cm}
		\begin{Large}
			Grado Universitario en Ingeniería Informática\\			
			 2020-2021\\ %Indica el curso académico
			\vspace{2cm}		
			\textsl{Trabajo Fin de Grado}
			\bigskip
			
		\end{Large}
		 	{\Huge ``Reconocimiento de Emociones en los Acompañantes de un Vehículo''}\\
		 	\vspace*{0.5cm}
	 		\rule{10.5cm}{0.1mm}\\
			\vspace*{0.9cm}
			{\LARGE Binxian Huang}\\ 
			\vspace*{1cm}
		\begin{Large}
			Tutor/es\\
			Jose Antonio Iglesias Martínez\\
			Leganés, Junio 2024\\
		\end{Large}
	\end{center}
	\vfill
	\color{black}
	% SI NUESTRO TRABAJO SE VA A PUBLICAR CON UNA LICENCIA CREATIVE COMMONS, INCLUIR ESTAS LÍNEAS. ES LA OPCIÓN RECOMENDADA.
	\includegraphics[width=4.2cm]{creativecommons.png}\\ %incluimos el logotipo de Creative Commons
	Esta obra se encuentra sujeta a la licencia Creative Commons \textbf{Reconocimiento - No Comercial - Sin Obra Derivada}
	\end{sffamily}
\end{titlepage}

\newpage %página en blanco o de cortesía
\thispagestyle{empty}
\mbox{}

%----------
%	RESUMEN Y PALABRAS CLAVE
%----------	
\renewcommand\abstractname{\large\bfseries\filcenter\uppercase{Resumen}}
\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{3}
	
	% ESCRIBIR EL RESUMEN AQUÍ

	El presente trabajo de fin de grado se centra en el desarrollo de un modelo de reconocimiento de emociones faciales para su aplicación en sistemas de vehículos, con el objetivo de mejorar la experiencia del usuario y aumentar la seguridad en la conducción. Aprovechando los avances en inteligencia artificial y aprendizaje automático, se propone un sistema capaz de identificar en tiempo real las emociones de los pasajeros y ajustar de manera dinámica las configuraciones internas del vehículo, tales como música, iluminación y temperatura, en función del estado emocional detectado.

	El proyecto aborda varias etapas clave: análisis del estado del arte, diseño del sistema, implementación del modelo, y evaluación de resultados. En el análisis del estado del arte se revisan las tecnologías actuales en la industria automotriz, incluyendo los Sistemas Avanzados de Ayuda a la Conducción (ADAS) y los vehículos autónomos, destacando la importancia de integrar la IA para una experiencia de usuario personalizada.

	El diseño del sistema incluye la identificación de requisitos funcionales y no funcionales, y la selección de herramientas y técnicas adecuadas para el procesamiento y análisis de datos. Para la implementación, se utilizó el dataset FER+ y se emplearon técnicas de preprocesamiento y redes neuronales convolucionales (CNN) para entrenar el modelo de reconocimiento emocional. Los resultados obtenidos indican una precisión razonable del modelo, con un porcentaje de aciertos del 70.14%, destacando la importancia de mejorar la calidad de las imágenes y el preprocesamiento para aumentar la precisión.

	Finalmente, se discuten los impactos socio-económicos y regulatorios del proyecto, subrayando la necesidad de cumplir con las normativas de protección de datos y garantizar la privacidad y seguridad de los datos personales. Este proyecto representa un avance significativo en la personalización y seguridad de los sistemas automotrices, aportando beneficios tanto para los usuarios como para la industria.
	
	\textbf{Palabras clave:}
	% Escribir las palabras clave aquí
	Reconocimiento de emociones, Vehículo, Pasajeros, Inteligencia Artificial, Aprendizaje Automático, Vehículos Inteligentes, Experiencia del Usuario, Seguridad en la Conducción, Sistemas Avanzados de Ayuda a la Conducción (ADAS), Redes Neuronales Convolucionales (CNN).
	
	\vfill
\end{abstract}
	\newpage % página en blanco o de cortesía
	\thispagestyle{empty}
	\mbox{}


%----------
%	DEDICATORIA
%----------	
\chapter*{Dedicatoria} % \chapter* evita que aparezca en el índice

\setcounter{page}{5}
	
	% ESCRIBIR LA DEDICATORIA AQUÍ	

	Quiero agradecer a mis padres, por su amor y apoyo en cada momento de mi vida, y por estar allí siempre en todo momento. 
	
	A mis amigos y compañeros, por brindarme su compañía y alegría en los momentos difíciles y en los más felices.
	
	A mis profesores, en especial a mi tutor Jose Antonio Iglesias Martinez, por su paciencia y orientación en la realización de este trabajo. 
	
	Y finalmente, a todas aquellas personas que, de una forma u otra, han contribuido a mi crecimiento personal y académico. 

		
	\vfill
	
	\newpage % página en blanco o de cortesía
	\thispagestyle{empty}
	\mbox{}
	

%----------
%	ÍNDICES
%----------	

%--
% Índice general
%-
\tableofcontents
\thispagestyle{fancy}

\newpage % página en blanco o de cortesía
\thispagestyle{empty}
\mbox{}

%--
% Índice de figuras. Si no se incluyen, comenta las líneas siguientes
%-
\listoffigures
\thispagestyle{fancy}

\newpage % página en blanco o de cortesía
\thispagestyle{empty}
\mbox{}

%--
% Índice de tablas. Si no se incluyen, comenta las líneas siguientes
%-
\listoftables
\thispagestyle{fancy}

\newpage % página en blanco o de cortesía
\thispagestyle{empty}
\mbox{}


%----------
%	MEMORIA
%----------	
\clearpage
\pagenumbering{arabic} % numeración con números arábigos para el resto de la memoria.	



	% COMENZAR A ESCRIBIR la MEMORIA
	
	% IMPORTANTE: en LaTeX hay una serie de caracteres especiales, que son: # $ % & \ ^ _ { } ~. Si aparecen en el texto, tendrás un error al compilar. La mayoría se pueden escapar escribiendo \ delante. Para \ utiliza \textbackslash ; para ^ \textasciitilde y para ~ \textasciicircum.

    % COMO incluir una FIGURA siguiendo las recomendaciones de la Guía: Alineación del título: izquierda, en la parte inferior de la figura; fuente a 10pt (el resto de la memoria está a 12); Numeración de la figura: se identificarán con números arábigos consecutivos tras la palabra Tabla. Si se sigue un esquema numérico de capítulos, el número de la tabla debe identificar con su primer dígito al capítulo, seguido por un punto y el número consecutivo que corresponda (Tabla 1.1, etc.); Propiedad intelectual: Se debe indicar la fuente de origen de la información en la parte inferior de la figura, a continuación del título.
    
    % EJEMPLO DE INCLUSIÓN de una figura:
    % \begin{figure}[H]
    % 	\ffigbox[\FBwidth] {
    % 	\caption[Nombre que aparecerá en el índice]{Nombre que aparecerá debajo de la figura}
    % 	}
    % 	{\includegraphics[scale=0.6]{archivo de la imagen; deberá estar en la carpeta de imágenes}}
    % \end{figure}
    
    % COMO incluir una TABLA siguiendo las recomendaciones de la Guía: Alineación del título: Centrado en la parte superior de de la tabla, en la parte superior de la tabla; fuente a 10pt (el resto de la memoria está a 12) y en mayúsculas; Numeración: se identificarán con números arábigos consecutivos tras la palabra Tabla. Si se sigue un esquema numérico de capítulos, el número de la tabla debe identificar con su primer dígito al capítulo, seguido por un punto y el número consecutivo que corresponda (Tabla 1.1, etc.); Propiedad intelectual: Se debe indicar la fuente de origen de la información en la parte inferior de la tabla.
    
    % EJEMPLO DE INCLUSIÓN de una tabla:
% \begin{table}[H]
% 	\ttabbox[\FBwidth]
% 	{\caption{Lorem ipsum}}
% 	{\begin{tabular}{|c|P{1.5cm}|c|P{1.5cm}|P{2cm}|c|P{1.5cm}|P{2cm}|}
% 		\hline
% 		\multicolumn{2}{|c|}{\textbf{I}} & \multicolumn{2}{c|}{\textbf{II}} & \multicolumn{3}{c|}{\textbf{III}} & \textbf{IV} \\
% 		\hline
% 		x & y & x & y & x & y & x & y \\
% 		\hline
% 		10.0 & 8.04 & 10.0 & 9.14 & 10.0 & 7.46 & 8.0 & 6.58 \\
% 		\hline
% 		8.0 & 6.95 & 8.0 & 8.14 & 8.0 & 6.77 & 8.0 & 5.76 \\
% 		\hline
% 		13.0 & 7.58 & 13.0 & 8.74 & 13.0 & 12.74 & 8.0 & 7.71 \\
% 		\hline
% 		9.0 & 8.81 & 9.0 & 8.77 & 9.0 & 7.11 & 8.0 & 8.84 \\
% 		\hline
% 		11.0 & 8.33 & 11.0 & 9.26 & 11.0 & 7.81 & 8.0 & 8.47 \\
% 		\hline
% 		14.0 & 9.96 & 14.0 & 8.10 & 14.0 & 8.84 & 8.0 & 7.04 \\
% 		\hline
% 		6.0 & 7.24 & 6.0 & 6.13 & 6.0 & 6.08 & 8.0 & 5.25 \\
% 		\hline
% 		4.0 & 4.26 & 4.0 & 3.10 & 4.0 & 5.39 & 19.0 & 12.50 \\
% 		\hline
% 		12.0 & 10.84 & 12.0 & 9.13 & 12.0 & 8.15 & 8.0 & 5.56 \\
% 		\hline
% 		7.0 & 4.82 & 7.0 & 7.26 & 7.0 & 6.42 & 8.0 & 7.91 \\
% 		\hline
% 		5.0 & 5.68 & 5.0 & 4.74 & 5.0 & 5.73 & 8.0 & 6.89 \\
% 		\hline
% 		\multicolumn{5}{l}{Fuente: BOE}
% 	\end{tabular}}
% \end{table}



\chapter{Introducción}

\section{Motivación}

Actualmente, siendo una era de grandes avances tecnológicos, la industria automotriz destaca como un faro de innovación, particularmente en el desarrollo de sistemas inteligentes diseñados para mejorar la seguridad y personalizar la experiencia del conductor. La integración de la Inteligencia Artificial (IA) en los sistemas de los vehículos representan un cambio transformador, pasando de los vehículos tradicionales a los “vehículos inteligentes” o smart vehicles. Estos vehículos no son meros medios de transporte, sino que son plataformas sofisticadas que se adaptan de forma dinámica a las necesidades y estados emocionales de los pasajeros. 

Las tecnologías en los vehículos actuales en gran medida no tienen en cuenta el contexto emocional de los pasajeros, dependen de una entrada en concreto como comandos táctiles o vocales. La implementación de sistemas capaces de percibir y reaccionar a los estados emocionales de los pasajeros podría ser un gran avance para el sector del diseño de software centrado en el usuario.

Este proyecto se centra en aprovechar el gran avance de la IA para desarrollar un modelo capaz de reconocer y analizar emociones faciales de los pasajeros en tiempo real, y aplicarlo en un sistema afectando a la forma en la que los vehículos interactúan con los ocupantes, ajustando configuraciones internas como la música, la iluminación y la temperatura, para mejorar el confort y la seguridad. Esto serviría mejorar la experiencia del usuario, además de que podría ser útil en los “Sistemas Avanzados de Ayuda a la Conducción” o Advanced Driver-Assistance System (ADAS), ya que ayudaría al conductor a librarse se realizar por sí mismo dichas acciones y no distraerse de la conducción, además que el modelo realizado también podría ser utilizado en los sistemas. 

\section{Objetivos}

El objetivo principal de este proyecto es desarrollar un modelo de reconocimiento de emociones faciales, que pueda ser aplicado en el sistema de un vehículo para mejorar la experiencia del usuario. Para ello, se plantean los siguientes objetivos específicos:

\begin{itemize}

	\item Desarrollar un modelo capaz de identificar las expresiones faciales mediante imágenes de las caras de los pasajeros.
    \item Diseñar un sistema que permita integrar el modelo con los demás sistemas del vehículo, además de poder interactuar con los controles del vehículo como la iluminación, los sistemas de audio o el control del clima.
    \item Mejorar las medidas de seguridad en función de los resultados del modelo, como permitir al sistema ajustar la dinámica de conducción, activar protocolos de seguridad, alertar sobre posibles peligros, etc. si se detectan señales de enfado o angustia. 
    \item Análisis y estudio de viabilidad económica y técnica del sistema, verificando si la integración de esta tecnología es factible para el mercado comercial, además de un análisis de impacto y una evaluación de los usuarios en la aceptación de esta tecnología en los vehículos.

\end{itemize}

\section{Estructura de la memoria}

El documento se organiza en siete capítulos principales:

\begin{enumerate}

    \item \textbf{Introducción:} En este capítulo se describe el contexto general del proyecto, incluyendo la motivación que impulsa a su realización y los objetivos específicos que se pretende alcanzar.
    \item \textbf{Estado del Arte:} En este capítulo se presenta una breve revisión literaria de los temas relacionados al proyecto, los avances de la experiencia del usuario el sector de los automóviles, la inteligencia artificial en la industria de la automoción, los ADAS y los sistemas implementados que mejoran las experiencias de los usuarios, además de un análisis de los  distintos trabajos previos en relación al tema. 
    \item \textbf{Diseño del sistema:} En este capítulo se describe el diseño y arquitectura del sistema, los requisitos necesarios, un análisis y explicación de la fuente de datos elegida para la realización del modelo, al igual que una descripción de las metodologías y herramientas que se usarán. 
    \item \textbf{Implementación y resultados:} En este capítulo se documenta el proceso práctico de la realización del modelo así como un análisis de los resultados obtenidos.
    \item \textbf{Marco regulador:} En este capítulo se realiza una revisión de las normativas y regulaciones relevantes que afecten a este proyecto. 
    \item \textbf{Entorno socio-económico:} En este capítulo se describe en una primera sección la planificación y administración del proyecto, y en una segunda sección el posible impacto del proyecto en la sociedad y la economía.

\end{enumerate}



\chapter{Estado del arte}

\section{Experiencia del Usuario en la Industria de la Automoción}

La experiencia de usuario (UX) es un concepto fundamental en el ámbito del diseño y desarrollo de productos y servicios, y su importancia ha ido creciendo exponencialmente en los últimos años. En el contexto de la industria automotriz, la UX se refiere a la manera que interactúan los usuarios con los vehículos, las tecnologías que incorporan estos mismos, y cómo esas interacciones afectan a la satisfacción del usuario.
\cite{userExperience}

Una buena experiencia de usuario no sólo implica una simple funcionalidad, ya sea ergonomía del asiento, integración de sistemas de entretenimiento, o características que mejoran la seguridad y comodidad; también implica tener en cuenta aspectos emocionales y psicológicos que influyen en la decisión de compra y la marca, como la facilidad de uso o la intuitividad de las interfaces de usuario. 

Al crecer la expectativa de los usuarios para una experiencia mejor y personalizable, conlleva a los fabricantes a tomar importancia el diseño centrado en el usuario con la realización de investigaciones de mercado y pruebas de usuario para comprender sus necesidades y deseos, lo que es beneficioso para la innovación y competitividad del sector. 

\subsection{Panales táctiles}

La interacción con los componentes de un automóvil moderno es muy diferente en comparación con unas décadas atrás. En el pasado, los paneles de los coches solo contenían controles físicos sencillos y la interacción era mediante botones, diales y deslizadores para ajustar la radio o el aire acondicionado. Además, las radios solo funcionaban mediante dispositivos físicos como casetes o CDs. Posteriormente evolucionaron a pequeñas pantallas digitales que añadían una funcionalidad de GPS muy sencilla, hasta evolucionar a lo que hay actualmente, pantallas táctiles que sustituyen e incluyen la mayoría de las funcionalidades de los botones físicos.
\cite{touchScreen}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{evolutionTouchscreen.png}
    \caption{Evolución del panel en un Volkswagen Golf \cite{userExperience}.}
    \label{fig:imagen1}
\end{figure}

Esta adaptación es debido al ràpido avance tecnológico de los teléfonos móviles y tablets, la comodidad en la que se interactúa pulsando o deslizando sobre las pantallas de los dispositivos, es lo que ha promovido a los fabricantes de automóviles a incorporar las mismas características de las pantallas táctiles en el lugar de los controles físicos de los coches.  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{usingTouchscreen.png}
    \caption{Conductor interaccionando con una pantalla táctil \cite{userExperience}.}
    \label{fig:imagen2}
\end{figure}

Las pantallas táctiles aportan bastantes beneficios, como pueden ser un diseño limpio y simplificado, siendo visualmente más atractivo al no contener tantos botones y diales, o flexibilidad y dinamicidad el la interfaz para añadir o mejorar funcionalidades.

Sin embargo, aunque permiten una interacción más sencilla y limpia, también presentan problemas de seguridad hacia el conductor. La distracción visual del conductor por la necesidad de desviar la vista de la carretera para interactuar con los paneles, lo que podría aumentar el riesgo de accidente o una falta de retroalimentación para las acciones que se realicen dificultaría al usuario manejar la pantalla sin mirar. Incluso en países como Reino Unido donde los vehículos tienen los volantes a la derecha, interaccionar con la pantalla con la mano izquierda podría ser difícil o incómodo. 

\subsection{Coches conectados}

El concepto de “coche conectado” surge de la integración de los coches en el Internet de las Cosas (IoT), al ser capaces de acceder a Internet, comunicarse con dispositivos inteligentes y recopilar datos en tiempo real. Inicialmente los automóviles contenían un número pequeño de Unidades de Control Electrónico (ECU), donde cada una era independiente de otra y realizaba una funcionalidad simple. Los coches de hoy en día pueden contener hasta 70 ECUs realizando muchas funcionalidades variadas, además de poder utilizar protocolos de comunicación que les permiten “hablar” con otros vehículos e infraestructuras, e incluso con Internet. Actualmente, incluso se ha logrado la integración de dispositivos móviles y smartphones en los paneles de los automóviles ofreciendo servicios avanzados de multimedia y entretenimiento. 
\cite{mobileIntegration}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{ecu.png}
    \caption{Ejemplos de ECUs en un coche \cite{mobileIntegration}.}
    \label{fig:imagen3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{connected_car_system.jpg}
	\caption{Visión general de un sistema de coche conectado \cite{mobileIntegration}.}
    \label{fig:imagen4}
\end{figure}

Al tener tantos dispositivos interconectados, las tecnologías y protocolos de comunicación en los coches conectados son fundamentales para su correcto funcionamiento, ya que permiten la interacción entre los componentes del vehículo, otros vehículos o las infraestructuras de las vías. Los diferentes tipos de comunicación que un coche conectado debe garantizar y las tecnologías disponibles para realizar dichas conexiones son las siguientes: 

\begin{itemize}

    \item \textbf{Vehicle To Sensors On Board communication (V2S) o Comunicación Intra-Vehicular:} esta comunicación se centra en la transmisión de información entre las ECUs y los sensores del vehículo, y se realiza mediante cables físicos o redes inalámbricas como Bluetooth o Ultra-Wideband. Un detalle a tener en cuenta es que aunque las redes inalámbricas ofrecen más versatilidad, presentan todavía problemas de seguridad y fiabilidad.
    \item \textbf{Vehicle To Vehicle Communication (V2V) o Comunicación Inter-Vehicular:} esta comunicación se centra en la transmisión de información entre diferentes coches sin la necesidad de un dispositivo remoto centralizado, utilizada principalmente para evitar accidentes, optimizar de rutas, compartir información multimedia y facilitar la interacción social entre conductores. Esta tecnología de comunicación enfrenta todavía desafíos ya una constante variación en la topología de la red o la presencia de obstáculos puede dificultar su gestión o interrupciones en el flujo de los datos.
    \item \textbf{Vehicle To Road Infrastructure Communication (V2R):} esta comunicación se centra en la transmisión de información entre un vehículo y una infraestructura de la vía compuesta por señales de tráfico, sensores de carretera y semáforos, y utilizada principalmente para una gestión eficiente del tráfico.
    \item \textbf{Vehicle To Internet Communication (V2I):} esta comunicación se centra en la transmisión de información entre un vehículo e Internet, utilizada principalmente para interaccionar con sus servicios y acceder a información multimedia. Se realiza mediante infraestructuras de red celular, al igual que un dispositivo móvil, utilizando una tarjeta SIM para permitir que el vehículo se conecte a una red 3G o 4G.

\end{itemize}

En cuanto a la integración de dispositivos móviles y smartphones con los coches conectados, se han desarrollado soluciones software capaces de permitir a la pantalla del coche acceder a los datos del dispositivo, e incluso permite al usuario controlar aplicaciones del dispositivo en la pantalla. Esta integración ofrece varias ventajas, como pueden ser una personalización diferente para cada usuario que depende de cada dispositivo, o además facilitar a los desarrolladores de aplicaciones crear aplicaciones sobre una plataforma genérica en vez de un modelo de pantalla específico. Las soluciones de integración más extendidas son: 

\begin{itemize}

    \item \textbf{MirrorLink:} desarrollado por el Car Connectivity Consortium (CCC), y permite la conexión mediante USB o Wi-Fi Direct. Crea directrices para la interfaz gráfica de las aplicaciones, con iconos de gran tamaño facilitando una navegación rápida y sencilla.
    \item \textbf{Applink:} desarrollada por Ford, y permite realizar distintas acciones dentro del vehículo como mostrar mensajes en la pantalla, reproducir música y ofrecer controles por voz.
    \item \textbf{Apple CarPlay:} solución desarrollada para dispositivos iPhone, extendiendo las funcionalidades del sistema estándar del vehículo, y permite controlar aplicaciones mediante la pantalla, comandos de voz con SIRI, y llamadas y mensajes con manos libres.
    \item \textbf{Android Auto:} solución desarrollada por la Open Automotive Alliance, y centrada en la conexión con dispositivos Android. Proporciona un tablero simple que permite el acceso a sistemas de navegación, como por ejemplo Google Maps, además de música, llamadas telefónicas, comandos de voz, etc.

\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{androidAuto.png}
	\caption{Ejemplos de interfaces de Android Auto en la pantalla del coche.}
	\label{fig:imagen5}
\end{figure}

Actualmente, de las tecnologías anteriores las más populares y mayoritariamente implementadas  son las dos últimas,  Apple CarPlay y Android Auto, debido a que tienen un mayor número de funcionalidades y tienen un mayor número de dispositivos con dicho sistema operativo compatible.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{integrationSolution.png}
	\caption{Ventas de coches con Apple CarPlay vs Android Auto \cite{mobileIntegration}.}
	\label{fig:imagen6}
\end{figure}

\subsection{Heads Up Display (HUD)}

Los sistemas Head-Up Display (HUD) fueron desarrollados originalmente para aviadores durante la Primera Guerra Mundial, los cuales servían para percibir aviones enemigos, y que posteriormente fueron evolucionando para mostrar más información como altitud, velocidad del viento, indicador de horizonte, etc. Actualmente se han introducido también en vehículos a tierra, los coches, mostrando datos esenciales sobre el parabrisas del vehículo.
\cite{hudPaper}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{hud.png}
    \caption{Vista de un HUD de un Jaguar \cite{hudPaper}.}
    \label{fig:imagen7}
\end{figure}

La estructura de un HUD se compone de tres componentes: el combinador, la superficie donde se proyecta la imagen; el proyector, el componente que genera y dirige la imagen hacia el combinador; y la unidad de procesamiento, que gestiona los datos obtenidos y controla la protección.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{hudWork.png}
    \caption{Funcionamiento y componentes de un HUD \cite{hudPaper}.}
    \label{fig:imagen8}
\end{figure}

La información que es proporcionada por un HUD de un automóvil está centrada en información relacionada con la conducción y el estado del vehículo: características de la carretera como límites de velocidad y condiciones del camino, o navegación con mapas y direcciones. Todo esto es útil para mantener al conductor informado, y proporcionar advertencias en caso de eventos peligrosos que puedan incurrir a la seguridad.

También se han desarrollado dispositivos HUD comerciales, que pueden instalarse de manera externa, aunque tienen limitaciones para usar la información de navegación al no ser integrados. 

Algunos vehículos modernos incluyen incluso funcionalidades como visión nocturna, mediante el uso de cámaras infrarrojas para mejorar la visibilidad en condiciones de poca luz, y proyectando esa información en el HUD. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{nigthHUD.png}
    \caption{Visión de cámara de infrarrojos en el HUD \cite{hudPaper}.}
    \label{fig:imagen9}
\end{figure}

También existen casos experimentales donde se implementan en los espejos retrovisores para proporcionar información de vistas traseras. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{retroHUD.png}
    \caption{Visión de HUD en espejos retrovisores \cite{hudPaper}.}
    \label{fig:imagen10}
\end{figure}

\section{Sistemas Avanzados de Ayuda a la Conducción (ADAS)}

Los Sistemas Avanzados de Asistencia al Conductor son un conjunto de tecnologías, combinando hardware y software, que ayudan a tener una conducción segura y eficiente. El objetivo principal es reducir el riesgo de accidentes y mejorar la experiencia de conducción mediante el uso de sensores, cámaras, radares y distintos dispositivos que monitorean el entorno alrededor del vehículo y avisan de manera activa al conductor. De esta manera, no sólo mejoran la seguridad de los ocupantes de un vehículo, también proporcionan más seguridad a otros ocupantes de las vías como pueden ser los peatones o ciclistas.
\cite{adasDGT}

Debido a la importancia y eficacia de estos sistemas para la seguridad, la Unión Europea aprobó en 2019 el Nuevo Reglamento de Seguridad 2019/2144 \cite{adasDGT} que recogía una serie de requisitos a cumplir para garantizar la seguridad de los vehículos y las personas. En España se implantó el 6 de julio de 2022 dicho Reglamento, y obliga a todos los coches nuevos de fábrica a incorporar al menos los siguientes ocho sistemas ADAS: 

\begin{itemize}

    \item \textbf{Detector de Fatiga y Somnolencia (DDR):} este sistema de ADAS se enfoca a prevenir distracciones del conductor debido a somnolencia, cansancio o falta de atención. Utiliza distintos tipos de tecnologías como: sensores en el volante para analizar la presión que se ejerce sobre el o la brusquedad de las correcciones de dirección; una cámara de reconocimiento facial que detecta el exceso de parpadeo o la visión de la mirada; o temporizador en el motor que recoge el tiempo que lleva en marcha. Si el sistema detecta anomalías o un tiempo de marcha largo, emite una señal o texto por los paneles del coche para avisar al conductor.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{adasDDR.jpg}
		\caption{Funcionamiento del sistema DDR \cite{adasObligatorios}.}
		\label{fig:imagen11}
	\end{figure}
	\item \textbf{Asistente de velocidad inteligente (ISA):} este sistema ADAS se enfoca en limitar la velocidad máxima a la que se circula mediante una cámara interior que lee las senales de las vías. La velocidad permitida es mostrada en el cuadro de mando del conductor, y el software actúa sobre el motor o el acelerador impidiendo que supere esa velocidad máxima, aunque se puede desconectar mediante un botón o pisando con fuerza el acelerador.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{adasISA.jpg}
		\caption{Funcionamiento del sistema ISA \cite{adasObligatorios}.}
		\label{fig:imagen12}
	\end{figure}
	\item \textbf{Cámara Trasera con Detección de Tráfico Cruzado (RCTA):} este sistema ADAS se enfoca en facilitar la visión del conductor en la marcha atrás con cámaras que muestran la imagen en el panel. También detecta posibles obstáculos que puedan estar en la trayectoria de la maniobra, advierte al conductor en la pantalla o en el retrovisor, y si el conductor no reacciona puede llegar a detener el coche de manera automática.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{adasRCTS.jpg}
		\caption{Funcionamiento del sistema RCTA \cite{adasObligatorios}.}
		\label{fig:imagen13}
	\end{figure}
	\item \textbf{Caja negra (EDR):} este sistema ADAS se enfoca en recopilar información, en caso de accidente, de los momentos anteriores y posteriores a él, para poder evaluar las posibles causas y estado de los sistemas del vehículo. El EDR está ubicado debajo del asiento del conductor, del tamaño aproximado de un teléfono móvil, y se puede conectar a un ordenador para obtener la información necesaria.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{adasEDR.jpg}
		\caption{Funcionamiento del sistema EDR \cite{adasObligatorios}.}
		\label{fig:imagen14}
	\end{figure}
	\item \textbf{Alerta de cambio involuntario de carril (LDW):} este sistema ADAS se enfoca en controlar que el vehículo no se desvía del carril por el que circula por una posible falta de atención del conductor. Utiliza una cámara frontal que detecta de forma continua las líneas del carril por el que circula, y en caso de que no se haya activado ningún intermitente y el vehículo esté saliendo del carril, avisa mediante una señal en el cuadro, un sonido o vibraciones. Además, si dispone de algún sistema de mantenimiento de carril, podría activarse para mantener el vehículo correctamente en el carril.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{adasLDW.jpg}
		\caption{Funcionamiento del sistema LDW \cite{adasObligatorios}.}
		\label{fig:imagen15}
	\end{figure}
	\item \textbf{Sistema de frenada de emergencia (BAS y EBA):} este sistema ADAS se enfoca en reducir el riesgo de colisión por alcance. Al realizar una frenada brusca, detecta si la intensidad con la que se pisa es la adecuada, y si no es suficiente el sistema interacciona frenando a la máxima intensidad posible. Además dispone de distintos sensores que permite distinguir a las personas y realiza un aviso, si el conductor no reacciona al aviso se activa el freno de manera automática.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{adasBASyEBA.jpg}
		\caption{Funcionamiento del sistema BAS y EBA \cite{adasObligatorios}.}
		\label{fig:imagen16}
	\end{figure}
	\item \textbf{Bloqueo de vehículo con alcoholímetro (Alcolock):} este sistema ADAS se enfoca en el control de la conducción tras la ingesta de alcohol. El vehículo dispone de un sistema de alcoholímetro, conectado al motor de arranque, y que impedirá al conductor arrancar el motor si la tasa de alcohol analizada por la boquilla sea superior al índice permitido. El sistema tendrá implementado sensores con tecnologías de reconocimiento de huellas dactilares, detección facial o fotografías digitales, que serán guardadas en memoria al igual que el historial de análisis, que servirá para identificar al usuario e impedir la falsificación o el fraude.
    \begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{adasAlcolock.jpg}
		\caption{Funcionamiento del sistema Alcolock \cite{adasObligatorios}.}
		\label{fig:imagen17}
	\end{figure}
	\item \textbf{Alerta de cinturón en las plazas traseras:} este sistema ADAS se enfoca en controlar y avisar el uso correcto del cinturón de seguridad en las plazas traseras. El sistema determina mediante un sensor de carga si la plaza está ocupada, y mediante un sensor de la hebilla si está bien cerrado. En caso de que el vehículo recorra una determinada distancia, un determinado tiempo o supere los 25 km/h y no se haya puesto el cinturón de una manera correcta, el sistema avisa automáticamente mediante señales visuales y acústicas.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{adasCinturon.jpg}
		\caption{Funcionamiento del sistema de Alerta de cinturón en las plazas traseras \cite{adasObligatorios}.}
		\label{fig:imagen18}
	\end{figure}

\end{itemize}

\section{Vehículos Autónomos}

El gran avance de la tecnología, en el ámbito del IoT y los sistemas embebidos, hacen que los vehículos se vuelvan cada vez más inteligentes, hasta llegar al punto de poder realizar acciones de forma autónoma sin intervención de las personas. Los vehículos autónomos (AV) tienen como objetivo mejorar la seguridad en las vías, facilitar las maniobras diarias de los conductores y disminuir el riesgo de accidentes debido a cansancio o fatiga, y optimizar recursos y tiempo de manera eficiente.
\cite{autonomousVehicles2}

Un vehículo autónomo puede clasificarse en diferentes niveles de autonomía, dependiendo del grado o acciones que llega a realizar el vehículo de forma autónoma en comparación con el conductor. Existe una normativa estandarizada, definida por la Sociedad de Ingenieros Automotrices (SAE), el estándar J3016 o Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems \cite{sae}, en el que se describe seis niveles de autonomía 
\cite{autonomousVehicles}:

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{saeLevels.jpg}
	\caption{Niveles de autonomía según el estándar SAE J3016 \cite{autonomousVehicles}.}
	\label{fig:imagen19}
\end{figure}

\begin{itemize}

    \item \textbf{SAE Nivel 0 (Sin Automatización):} en este nivel, el conductor es responsable de todas las acciones relacionadas a la conducción, como son la aceleración, el frenado, el manejo de dirección y el uso de señales. El vehículo puede tener instalado sistemas de advertencia, como alertas de colisión o de objetos en punto ciego, pero en ningún caso realiza alguna acción controlando el vehículo por el conductor.
    \item \textbf{SAE Nivel 1 (Asistencia al Conductor):} en este nivel, se dispone de sistemas que pueden controlar funciones específicas del vehículo, como la aceleración (control de crucero adaptativo), el frenado (frenado de emergencia) o el manejo de direccion (sistema de mantenimiento de carril). El conductor debe estar atento y en supervisión constante para tomar el control de vuelta de los pedales en caso de necesidad.
    \item \textbf{SAE Nivel 2 (Automatización Parcial):} en este nivel, el vehículo puede además controlar la dirección de forma autónoma y simultánea con la aceleración y el frenado, pero el conductor debe estar al igual en supervisión constante sobre el entorno y tomar el control en caso necesario.
    \item \textbf{SAE Nivel 3 (Automatización Condicional):} en este nivel, el vehículo es capaz de percibir el estado del entorno de manera autónoma, y controlar todas las funciones de conducción en determinadas condiciones simples, como en autopistas. Aun así, en este caso el conductor debe estar en supervisión para tomar el control si el sistema lo solicita.
    \item \textbf{SAE Nivel 4 (Alta Automatización):} en este nivel, el vehículo puede encargarse de todas las funciones de forma autónoma en situaciones específicas y bien definidas, como por ejemplo en autopistas o áreas urbanas. El conductor sigue necesitando tener presencia en determinadas circunstancias, pero en este nivel ya existe la posibilidad de no utilizar volante ni pedales.
    \item \textbf{SAE Nivel 5 (Automatización Completa):} en este nivel, el vehículo es capaz de encargarse de todas la funciones en cualquier situación sin la necesidad de presencia humana.

\end{itemize}

Los vehículos autónomos están equipados con una gran cantidad de sensores y sistemas que les permiten lograr la percepción del entorno y poder conducir de una manera segura 
\cite{autonomousVehicles}:

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{saeSensors.png}
	\caption{Sensores en un vehículo autónomo \cite{autonomousVehicles}.}
	\label{fig:imagen20}
\end{figure}

\begin{itemize}

    \item \textbf{Light Detection and Ranging (LIDAR):} sensor que funciona mediante detección láser, analizando la luz reflejada que se envía. Este sensor es capaz de detectar los objetos de alrededor del vehículo y crear mapas virtuales de 2D o 3D.
    \item \textbf{Radio Detection and Ranging (RADAR):} sensor que funciona mediante ondas electromagnéticas que detectan los obstáculos o vehículos cercanos. Pueden ser de largo alcance, medio alcance o corto alcance, y se usan en sistemas como el control de crucero adaptativo, la advertencia de colisión o la detección del ángulo muerto.
    \item \textbf{Global Navigation Satellite System (GNSS):} conjunto de sistemas que funcionan mediante señal de los satélites, como por ejemplo el GPS, y proporciona información precisa sobre la ubicación y la hora.
    \item \textbf{Sensores Ultrasónicos:} sensores que detectan objetos en una distancia muy cercana al vehículo, utilizados en sistemas de asistencia al estacionamiento.
    \item \textbf{Unidad de Medición Inercial (IMU):} sensor que mide el movimiento lineal y angular del vehículo utilizando acelerómetros y giroscopios. La información que recibe se combina con los datos del sistema GPS para obtener una mayor precisión en la posición.
    \item \textbf{Cámaras térmicas:} cámaras que detectan el calor, con un rango más amplio que los faros, y son útiles para la detección de obstáculos, ya sean personas, animales u otros coches, en condiciones difícilmente perceptibles, como por la noche, en zona de humo o con un brillo solar intenso.
    \item \textbf{Cámaras de video:} cámaras de alta resolución, que monitorean y transmiten de manera constante imágenes de la vía, usadas para procesar información relevante como señales de tráfico, peatones o ciclistas.

\end{itemize}

El conjunto de sensores, junto a los sistemas ADAS, son lo que permiten a los conductores circular de manera más segura, y lo que hace posible que los vehículos autónomos puedan realizar tareas de conducción sin la intervención del conductor.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{adasSensorWorkflow.png}
	\caption{Clasificación de sensores usados en sistemas ADAS \cite{adasSensors}.}
	\label{fig:imagen21}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{adasSensorRange.png}
	\caption{Rangos sobre los que actúan los sensores \cite{adasSensors}.}
	\label{fig:imagen22}
\end{figure}

\section{Inteligencia Artificial y Aprendizaje Automático}

La Inteligencia Artificial (IA) se define como “capacidad de las máquinas para imitar la inteligencia de las personas humanas en la realización de tareas” \cite{deepLearning}. La investigación en IA está centrada en el desarrollo de dispositivos que perciben su entorno, resuelven problemas, y aprenden de los errores que sirvan posteriormente, para alcanzar un objetivo específico.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{sectionsAI.png}
	\caption{Campos de la Inteligencia Artificial \cite{artInt}.}
	\label{fig:imagen23}
\end{figure}

El Aprendizaje Automático (ML), por otra parte, es un campo de la IA que está centrado en el desarrollo de esos algoritmos que les permiten aprender y hacer predicciones a partir de los datos o problemas. Es definido como “capacidad de las máquinas para aprender sin ser programadas específicamente” \cite{deepLearning}. En la práctica, se puede definir como la construcción de modelos matemáticos mediante unos datos de muestra, y realizar con esos modelos predicciones o tomar decisiones sobre datos nuevos. 
\cite{deepLearning}

Los métodos de ML más comunes que se utilizan para aprender y realizar predicciones son los siguientes 
\cite{deepLearning} \cite{artInt}:

\begin{itemize}

    \item \textbf{Aprendizaje Supervisado (SL):} método en el que se utilizan datos etiquetados para el entrenamiento del modelo. Esto quiere decir que para cada dato de entrada existe una salida final que es conocida, y el objetivo del modelo es aprender a unir cada entrada con su salida. Tras tener el modelo entrenado, este será capaz de predecir las salidas no conocidas de nuevas entradas.
    \item \textbf{Aprendizaje No Supervisado (UL):} método en el que se utilizan datos no etiquetados para el entrenamiento del modelo. En este caso, el modelo pretende encontrar patrones ocultos o estructuras en los datos de entrada sin conocer la salida.
    \item \textbf{Aprendizaje Semi-Supervisado (SSL):} método en el que se utiliza una combinación de datos etiquetados y no etiquetados para el entrenamiento del modelo. Este método es útil en caso de que la obtención de los datos etiquetados sea costosa.
    \item \textbf{Aprendizaje por refuerzo (RL):} método en el que se entrena un agente, que interactúa con el entorno realizando determinadas acciones y obteniendo recompensas o penalizaciones dependiendo del resultado de la acción, y aprende con el objetivo de maximizar la recompensa acumulada. Es útil en campos como la robótica, los juegos o la navegación autónoma.
    \item \textbf{Aprendizaje Profundo (DL):} conjunto de métodos más complejos que utilizan redes de neuronas artificiales con múltiples capas ocultas para aprender patrones en un conjunto de datos muy grande. Las arquitecturas de DL más conocidas son redes neuronales recurrentes (RNN) y redes neuronales convolucionales (CNN).

\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{modelsML.png}
	\caption{Modelos de Aprendizaje Automático \cite{artInt}.}
	\label{fig:imagen24}
\end{figure}

\subsection{Redes Neuronales}

Una red de neuronas artificial (ANN), o simplemente red de neuronas, es un sistema basado en múltiples nodos llamados neuronas, distribuidas en varias capas. Las capas principales de una red de neuronas son la capa de entrada que recibe los datos, las capas ocultas que procesan los datos, y la capa de salida que devuelve los resultados. 
\cite{annReview}

Cada neurona de una capa recibe entradas, procesa esas entradas y devuelve una salida que es enviada a la capa siguiente. El proceso de un dato en una neurona se realiza a través de una función de activación, función que introduce características no lineales en la red para que pueda aprender relaciones complejas entre el dato de entrada y salida.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{annStructure.png}
	\caption{Artificial Neural Network basic architecture \cite{annReview}.}
	\label{fig:imagen25}
\end{figure}

\subsubsection{Redes Neuronales Recurrentes (RNN)}

Las redes de neuronas recurrentes (RNN) son un tipo de red neuronal que destacan por tener conexiones en forma de ciclos en la red, permitiéndoles mantener una “memoria” de los estados anteriores, por lo que son eficaces con el tratamiento de tareas donde tiene importancia el contexto temporal. 
\cite{artInt} \cite{annReview}

Las RNN procesan los datos de manera secuencial: en cada estado de procesamiento, se toma un valor de entrada y su estado anterior, y a partir de ambas genera una salida y actualiza su estado. Este modo de procesamiento en forma de ciclo es lo que permite a la red mantener la “memoria” de la información a lo largo de todo el proceso, y lo que es relevante para tareas como el procesamiento del lenguaje natural (NLP), donde la comprensión de una palabra depende del contexto anterior.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{rnnStructure.png}
	\caption{Recurrent Neural Network basic architecture \cite{annReview}.}
	\label{fig:imagen26}
\end{figure}

\subsubsection{Redes Neuronales Convolucionales (CNN)}

Las  redes de neuronas convolucionales (CNN) son un tipo de red neuronal que destacan por poder procesar y analizar datos que tengan una estructura de cuadrícula, como por ejemplo las imágenes. 
\cite{artInt} \cite{annReview}

Una CNN está compuesta por varias capas de distintos tipos que transforman la entrada recibida de una manera específica:

\begin{itemize}

    \item \textbf{Capa convolucional:} capa que aplica filtros o núcleos a los datos de entrada, siendo el filtro una matriz pequeña que se desliza sobre el dato, y realiza una convolución sobre los datos para obtener un mapa de características. Estas características son las que permiten a la red detectar los patrones en la imagen, como bordes o texturas.
    \item \textbf{Capa de pooling o subsampling:} capa que aplica una reducción de dimensionalidad sobre el mapa de características, generalmente siendo una operación de max-pooling tomando el valor máximo de cada región. Esta capa sirve para reducir la cantidad de parámetros y controlar el sobreajuste.
    \item \textbf{Capas completamente conectadas:} capas que se sitúan al final de la red, y sirven para procesar las características obtenidas de las capas anteriores y realizar una clasificación final. Cada neurona de esta capa se conecta a todas las neuronas de la capa anterior.

\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{cnnStructure.png}
	\caption{Convolutional Neural Network basic architecture \cite{annReview}.}
	\label{fig:imagen27}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{basicCnn.png}
	\caption{Convolutional Neural Network layers visualization \cite{deepImages}.}
	\label{fig:imagen28}
\end{figure}

\section{Trabajos Relacionados}

El reconocimiento de emociones ya sea de los ocupantes o del conductor en un vehículo es un área de desarrollo fundamental para la industria automotriz. Una capacidad para identificar y responder eficaz puede mejorar considerablemente la seguridad o la experiencia de usuario de los ocupantes del vehículo. Al ser un área de investigación que ha ido avanzado a lo largo de los años, se han realizado muchos estudios destacados, llegando al estado actual con sistemas tecnológicos cada vez más sofisticados.

Un proyecto interesante y reciente es “Drivers’ Comprehensive Emotion Recognition Based on HAM” \cite{rw1} realizado en 2023, basado en mejorar la precisión de modelos de aprendizaje profundo para reconocer emociones, en este caso del conductor, utilizando señales audiovisuales. La metodología utilizada es una combinación híbrida de diferentes redes neuronales, y un entrenamiento y validación con distintos conjuntos de datos, y lograron una precisión del 79.60\% en el conjunto de datos FER2013 superando a otros modelos ya existentes.

Otro proyecto destacado es “AI-Powered In-Vehicle Passenger Monitoring Using Low-Cost mm-Wave Radar” \cite{rw2} publicado en 2021, en el que se desarrolla un sistema de detección y monitoreo de los pasajeros de un vehículo. Concretamente, trata de identificar los asientos ocupados y contar el número de pasajeros que hay mediante un radar de onda milimétrica generando mapas de calor, y procesando la información con algoritmos de aprendizaje automático. En comparación con los sistemas tradicionales, este sistema permite detectar pasajeros con más precisión, como por ejemplo al detectar múltiples pasajeros en asientos adyacentes, además de ser una solución económica con el uso de radares de bajo costo.

En cuanto a proyectos en relación a la experiencia de usuario, un proyecto destacado es “Mobility Prediction-Based Optimisation and Encryption of Passenger Traffic-Flows Using Machine Learning” \cite{rw3} publicado en 2020. Este proyecto se centra en desarrollar un modelo predictivo de aprendizaje automático para mejorar el rendimiento de infraestructuras ferroviarias, en este caso del metro de Londres, abordando problemas como la congestión, los accidentes, la sobrecarga de vagones o el mantenimiento. Utilizan sensores de identificación por radiofrecuencia (RFID) que registran los datos de pasajeros en tiempo real mediante tecnología 5G, y con un modelo de redes neuronales artificiales predicen los patrones de flujo de los pasajeros. Además todos los datos enviados en tiempo real son encriptados lo que asegura su seguridad y privacidad. Realizaron simulaciones con datos reales del metro de Londres y el modelo alcanzó una precisión del 91.17\% en la predicción del flujo de movilidad de los pasajeros.

Otro ejemplo de proyecto es “Estimating passenger experience from vehicle data: Preconditions for using machine learning” \cite{rw4} publicado en 2020. Este artículo se centra en evaluar métodos de aprendizaje automático para estimar de forma automática la experiencia de los pasajeros en los autobuses eléctricos, utilizando datos obtenidos con sensores en el autobús y encuestas a los pasajeros sobre la experiencia del viaje. Las implicaciones de este estudio puede ser interesante para los fabricantes de vehículos, ya que podrían mejorar su diseño y desarrollo, o para los operadores de los transportes públicos, ya que podrían mejorar la formación de conductores o la planificación de rutas en función de la satisfacción del pasajero. 

Otro proyecto, también relacionado a mejorar la experiencia de usuario sobre el uso de autobuses como medio de transporte público, es “A Real-Time Machine Learning-Based Public Transport Bus-Passenger Information System” \cite{rw5} publicado en 2021. El objetivo de este estudio es desarrollar un sistema de información en tiempo real que muestra la ubicación de los autobuses para mejorar su puntualidad y confiabilidad, y optimizar la asignación de recursos mediante técnicas de machine learning, usando datos diarios para predecir demandas de pasajeros en diferentes rutas. Esto servirá para estimar de una manera más exacta el tiempo de llegada de un autobús y por lo tanto optimizar tiempo, además de asignar de una manera más eficiente autobuses a las diferentes rutas que lo necesiten.

\section{Conclusiones y Análisis de Resultados}

Tras haber realizado un análisis e investigación del Estado del Arte y los proyectos realizados en relación al tema, se ha podido observar que en la evolución del automóvil el factor de la seguridad es un papel muy importante e innovador, sobre todo en las tecnologías ADAS. Esto es debido sobre todo a los avances tecnológicos de los sensores, las mejoras en las capacidades de cómputo y el desarrollo de algoritmos más avanzados de IA, todo esto de relación muy importante si en un futuro se quiere transicionar de vehículos tradicionales a vehículos autónomos.

La mayoría de los sistemas ADAS actualmente están diseñados para asistir al conductor, mientras que no hay muchos que sean enfocados a los pasajeros. Pero la experiencia del usuario, ya sea centrada en la seguridad como en la comodidad, debería tener igual importancia, y más en los vehículos autónomos donde la presencia del conductor será cada vez menor.

Tras obtener estas conclusiones del análisis, con este proyecto se pretende proponer un sistema, cuyo diseño se centra en mejorar la experiencia de usuario de los ocupantes de un vehículo, y que funcione a partir de análisis en tiempo real de sus emociones y reaccionar de una manera u otra interactuando con las funcionalidades de los vehículos, como son el sonido de la música o radio, el aire acondicionado, la comodidad de los asientos, o integrarse en sistemas ADAS que sirvan para mejorar la experiencia de los ocupantes.



\chapter{Diseño del sistema}

Al igual mencionado anteriormente, los avances hoy en día en la industria automotriz sobre los sistemas de seguridad y experiencia al usuario están mayoritariamente enfocados al conductor, y las investigaciones y proyectos en los que se tienen en cuenta a los acompañantes son relacionados al sector de los transportes públicos. 

Por el anterior motivo, se propone un sistema cuyo diseño está enfocado para los ocupantes de un vehículo privado. Concretamente, el uso principal del sistema es mejorar la experiencia de un ocupante mediante el control automático de funcionalidades de multimedia de un vehículo dependiendo del estado emocional en el que se encuentren los ocupantes.

El sistema tambien estaria abierto a posibles otras aplicaciones en sistemas ADAS, como por ejemplo restringir la velocidad máxima que puede alcanzar un vehículo en caso de que el estado emocional de un acompañante sea de enfado o estrés, ya que podría influir en el comportamiento del conductor, y los sistemas actuales solo están implementados para detectar el estado del conductor y no de los acompañantes. Pero esto es solo una posible idea de aplicación posible, que no se abordará en este proyecto. 

Por lo tanto, la arquitectura de nuestro sistema estará compuesta de diferentes módulos clave, cada uno con una funcionalidad separada, y que juntos forman el sistema final que permitirá el reconocimiento de emociones y la adaptación del resultado sobre las funcionalidades del vehículo:

\begin{enumerate}

    \item \textbf{Módulo de obtención de imágenes en tiempo real}
    \begin{itemize}
		\item Las imágenes se obtendrán mediante cámaras de alta resolución instaladas en los coches para capturar imágenes de las caras  y emociones de los acompañantes. El software para su funcionamiento se encargará de la frecuencia de captura, la calidad de captura, la cantidad de imágenes, y el pre-procesamiento y envío en tiempo real de las imágenes, ya sea por el bus CAN del vehículo o por conexión inalámbrica.
	\end{itemize}

    \item \textbf{Módulo de predicción y detección de emociones}
    \begin{itemize}
		\item Las imágenes recibidas serán analizadas por un modelo de red neuronal convolucional (CNN) pre-entrenado que detectará las emociones de esas imágenes, y el resultado será enviado a la API o interfaz de software encargada de la interacción con el vehículo. 
	\end{itemize}
    
    \item \textbf{Módulo de interacción}
    \begin{itemize}
		\item El resultado obtenido de los análisis de las imágenes será utilizado para decidir las acciones a realizar, y se comunicara con el sistema de interacción, que controlará las funcionalidades del vehículo según el resultado: por ejemplo si los acompañantes están alegres subir el volumen de la música y activas las luces de ambiente, si están tristes cambiar a una música más animada, si están enfadados bajar el volumen de la música, si están cansados o durmiendo bajar la música y el aire acondicionado, etc. Dependiendo de cada una de las posibilidades que prediga en modelo, podrían haber diferentes combinaciones, incluso podría ser personalizable por el mismo usuario.
	\end{itemize}

\end{enumerate}

\section{Análisis de requisitos}

El sistema deberá cumplir una serie de requisitos funcionales, que definen las funcionalidades y características que el sistema debe tener para alcanzar los objetivos del proyecto, y no funcionales que definen el comportamiento del sistema enfocado a la calidad de servicio y funcionamiento.

Los requisitos se definen en formato de tabla, siendo cada requisito identificado por RF si es funcional y RNF si es no funcional, seguido de un número de requisitos. Cada definición de requisito tendrá un nombre, una descripción, y tres propiedades del requisito: importancia del requisito (alta, media o baja), prioridad del requisito (alta, media o baja) y complejidad del requisito (alta, media o baja).

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Plantilla de tabla de requisitos}}
	{
	  \begin{tabular}{|c|c|c|c|}
		\hline
		\multicolumn{4}{|c|}{\cellcolor{gray}\textbf{RY-XX}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & \multicolumn{3}{c|}{} \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & \multicolumn{3}{c|}{} \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta & Media & Baja \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta & Media & Baja \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta & Media & Baja \\
		\hline
	  \end{tabular}
	}
\end{table}

\subsection{Requisitos funcionales}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-01}}
	{
	  \begin{tabular}{|c|P{10cm}c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-01}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Captura de imágenes \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Capturar imágenes de los ocupantes en tiempo real. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-02}}
	{
	  \begin{tabular}{|c|P{10cm}c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-02}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Frecuencia de captura \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Ajustar la frecuencia de captura para optimizar rendimiento y precisión. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-03}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-03}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Calidad de captura \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Utilizar cámaras de alta resolución para la claridad de las imágenes. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-04}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-04}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Pre-procesamiento de imágenes \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Realizar preprocesamiento para enfocar la imagen sobre la cara. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-05}}
	{
	  \begin{tabular}{|c|P{10cm}c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-05}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Envío de imágenes \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Enviar imágenes en tiempo real al modelo de predicción. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-06}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-06}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Análisis de imágenes \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Analizar las imágenes con el modelo de CNN pre-entrenado. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-07}}
	{
	  \begin{tabular}{|c|P{10cm}c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-07}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Tipos de emociones \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Detectar emociones correspondientes \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-08}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-08}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & API de resultados \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Comunicar el resultado de la predicción a la API de interacción. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-09}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-09}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Acciones según emoción \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Ejecutar acciones en respuesta al resultado obtenido. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-10}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-10}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Registro de acciones \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Registrar acciones realizadas y emociones detectadas. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-11}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-11}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Personalización de acciones \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Permite personalización de acciones por parte del usuario. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Baja \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-12}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-12}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Compatibilidad de vehículos \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & El sistema tendrá que ser compatible con diferentes vehículos. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-13}}
	{
	  \begin{tabular}{|c|P{10cm}c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-13}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Integración con otros sistemas \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Integrar con otros sistemas de información. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Baja \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Baja \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito funcional RF-14}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RF-14}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Mantenimiento remoto \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Soportar mantenimiento remoto y solución de problemas. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\subsection{Requisitos no funcionales}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito no funcional RNF-01}}
	{
	  \begin{tabular}{|c|P{10cm}c|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RNF-01}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Tiempo de respuesta \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & El sistema deberá procesar la respuesta en tiempo real. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito no funcional RNF-02}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RNF-02}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Escalabilidad \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & El sistema debe poder escalar para manejar múltiples usuarios a la vez. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito no funcional RNF-03}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RNF-03}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Robustez \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & El sistema deberá poder manejar errores o fallos sin interrumpir el funcionamiento. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito no funcional RNF-04}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RNF-04}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Seguridad de datos \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & Garantizar la seguridad de los datos de imágenes y emociones. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito no funcional RNF-05}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RNF-05}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Cumplimiento de normativa \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & El sistema deberá cumplir la normativa de protección de datos y privacidad. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito no funcional RNF-06}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RNF-06}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Eficiencia energética \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & El sistema deberá ser eficiente en el consumo de energía. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Media \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Media \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Requisito no funcional RNF-07}}
	{
	  \begin{tabular}{|c|P{10cm}|}
		\hline
		\multicolumn{2}{|c|}{\cellcolor{gray}\textbf{RNF-07}} \\
		\hline
		{\cellcolor{gray}\textbf{Nombre}} & Latencia \\
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & El sistema debe tener una latencia de procesamiento mínima para asegurar una buena experiencia de usuario. \\
		\hline
		{\cellcolor{gray}\textbf{Necesidad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Prioridad}} & Alta \\
		\hline
		{\cellcolor{gray}\textbf{Complejidad}} & Alta \\
		\hline
	  \end{tabular}
	}
\end{table}

\section{Análisis de datos}

Como se ha mencionado anteriormente, el sistema obtiene los datos a partir de imágenes en tiempo real. Estas imágenes podrían ser recopiladas por cámaras instaladas dentro del vehículo, y posteriormente preprocesadas por Visión Artificial (Computer Vision) u otra técnica para obtener una imagen más detallada de la cara, y adaptadas al formato que se necesite, y será procesada posteriormente por el modelo de aprendizaje automático.

En este caso, para el entrenamiento del modelo se usará un conjunto de datos publicado en Internet, concretamente “FER+ (FER2013Plus)” \cite{fer2013Plus} publicada por Microsoft en su Github \cite{fer2013PlusMicrosoft}, una versión extendida y mejorada del dataset original “Facial Expression Recognition (FER2013)” \cite{fer2013}, publicado en Kaggle como enunciado de la competición “Challenges in Representation Learning: Facial Expression Recognition Challenge”.

El dataset FER2013 consta de 35.887 imágenes en escala de gris de caras humanas, con una resolución de 48 x 48 píxeles, adecuada con un balance entre calidad de visión y eficiencia computacional. Las imágenes tienen 7 categorías de emoción representadas en columnas y con valores numéricos dependiendo de la cantidad de presencia de esa emoción en la imagen, sumando un total de 10. La clasificación de las emociones son: enfado (anger), disgusto (disgust), miedo (fear), felicidad (happy), tristeza (sad), sorpresa (surprise) u neutral. Todos los datos de este conjunto se obtuvieron de la recopilación de imágenes de Internet y anotación manual por parte de personas que clasificaron las imágenes sobre las 7 categorías mencionadas anteriormente.

En cuanto a la versión extendida FER+, tiene mejoras del dataset original en relación a la reanotacion de las imágenes de una manera más precisa, eliminando imágenes de baja calidad, cambiando anotaciones de imágenes incorrectamente anotadas, reduciendo el ruido y mejorando la calidad general del dataset. Además añades 3 categorías más para el grado de clasificación de las imágenes, que son desprecio (contempt), desconocido (unknown) y NT (not a face). Esta es la causa de la elección de este dataset, al contener una cantidad amplia de datos, con un etiquetado más preciso y mejorado que el original.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{FER+vsFER.png}
	\caption{Ejemplos de reanotación de imagenes (arriba FER2013 original, abajo FER+) \cite{fer2013PlusMicrosoft}.}
	\label{fig:imagen29}
\end{figure}

\section{Análisis de sistemas y software utilizado}

Para la implementación y análisis del modelo de aprendizaje automático, se harán uso de diferentes programas informáticos y herramientas:

\begin{itemize}

    \item \textbf{Python:} es un lenguaje de programación de alto nivel que destaca en el ámbito de la inteligencia artificial (IA) y el aprendizaje automático (ML) por su integración con una gran cantidad de librerías especializadas para el manejo de grandes volúmenes de datos, además de simplificar la creación, el entrenamiento y la evaluación de modelos predictivos de ML. \cite{python}
    \item \textbf{Keras:} es una librería de Python que facilita el desarrollo de modelos de aprendizaje profundo con redes neuronales, además de proporcionar módulos para procesar imágenes, texto y secuencias de datos. \cite{keras}
    \item \textbf{NumPy y Pandas:} son dos librerías fundamentales que se utilizan para la interacción con datos numéricos. NumPy (Numerical Python) es destacado por una manipulación rápida y eficiente de grandes volúmenes de datos, soportando incluso datos en forma de arrays multidimensionales y matrices, y operación con una amplia cantidad de funciones matemáticas. \cite{numpy} Pandas es una librería construida sobre NumPy, y destaca por una manipulación de datos eficiente con Series y DataFrames, estructuras unidimensionales la primera y bidimensionales la segunda, similar a columnas o tablas de una hoja de cálculo. También incluye funciones capaces de importar y exportar datos a diferentes tipos de archivos como Excel o CSV. \cite{pandas}
	\item \textbf{Matplotlib y Seaborn:} son dos librerías centradas en la visualización de datos. Matplotlib destaca por un control detallado sobre la creación de distintos tipos de gráficos simples y complejos, proporcionando flexibilidad para modificar prácticamente todos los aspectos de los gráficos, pero a costa de mayor complejidad. \cite{matplotlib} Seaborn por otro lado es una librería construida sobre Matplotlib, que simplifica la creación de gráficos y visualizaciones, y facilita la exploración y visualización de los datos. \cite{seaborn}

\end{itemize}



\chapter{Implementación y resultados}

\section{Preprocesado de datos}

Como se ha comentado anteriormente, el dataset que se usará es el FER+. El dataset tiene las imágenes etiquetadas y clasificadas en conjuntos de entrenamiento, validación y test. Tras descargar los datos y realizar un análisis, se obtiene la siguiente distribución de valores:

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Tabla de distribución inicial de valores}}
	{
	  \begin{tabular}{|P{9cm}|P{4cm}|}
		\hline
		{\cellcolor{gray}\textbf{Emoción}} & {\cellcolor{gray}\textbf{Cantidad}} \\
		\hline
		Neutral & 12.906 \\
		\hline
		Happiness & 9.355 \\
		\hline
		Surprise & 4.462 \\
		\hline
		Sadness & 4.371 \\
		\hline
		Anger & 3.111 \\
		\hline
		Disgust & 248 \\
		\hline
		Fear & 819 \\
		\hline
		Contempt & 216 \\
		\hline
		Unknown & 222 \\
		\hline
		Not a face & 4 \\
		\hline
		{\cellcolor{gray}\textbf{Total}} & 35.714 \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{distribucionInicial.png}
	\caption{Gráfico de distribución inicial de valores.}
	\label{fig:imagen30}
\end{figure}

Se puede apreciar que los datos son muy desbalanceados, los datos que más hay son de etiqueta “neutral” y “happiness”, mientras que de “not a face” hay solo 4. Se decide que los datos de “not a face” y “unknown” se desechan ya que no son relevantes y no aportan valor. Para balancear los datos, también se decide desechar “disgust”, “contempt” y “fear”, ya que en comparación también son datos bastante desbalanceados, además de que son parecidos a “sadness”, “anger” y “surprise”. Por último, se eliminan entradas de “neutral” y “hapiness” ya que tienen un número de entradas mucho mayor que los demás, quedando al final con una cantidad de entre 3.000 y 5.000:

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Tabla de distribución balanceada de valores}}
	{
	  \begin{tabular}{|P{9cm}|P{4cm}|}
		\hline
		{\cellcolor{gray}\textbf{Emoción}} & {\cellcolor{gray}\textbf{Cantidad}} \\
		\hline
		Neutral & 4.906 \\
		\hline
		Happiness & 4.355 \\
		\hline
		Surprise & 4.462 \\
		\hline
		Sadness & 4.371 \\
		\hline
		Anger & 3.111 \\
		\hline
		{\cellcolor{gray}\textbf{Total}} & 21.205 \\
		\hline
	  \end{tabular}
	}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{distribucionBalanceada.png}
	\caption{Gráfico de distribución balanceada de valores.}
	\label{fig:imagen30}
\end{figure}

También se muestra el número de valores de cada tipo de cada conjunto clasificado en entrenamiento, validación y test:

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Tabla de costes hardware}}
	{
	  \begin{tabular}{|c|P{1.5cm}|P{1.5cm}|P{1.5cm}|P{1.5cm}|P{1.5cm}|P{1.5cm}|}
		\hline
		{\cellcolor{gray}\textbf{Conjunto}} & {\cellcolor{gray}\textbf{Neutral}} & {\cellcolor{gray}\textbf{Happiness}} & {\cellcolor{gray}\textbf{Surprise}} & {\cellcolor{gray}\textbf{Sadness}} & {\cellcolor{gray}\textbf{Anger}} & {\cellcolor{gray}\textbf{Total}} \\
		\hline
		Entrenamiento & 4.009 & 3.428 & 3.562 & 3.515 & 2.467 & 16.981 \\
		\hline
		Validación & 435 & 499 & 456 & 412 & 319 & 2.121 \\
		\hline
		Test & 462 & 428 & 444 & 444 & 325 & 2.103 \\
		\hline
		\multicolumn{6}{c|}{\cellcolor{gray}\textbf{Total}} & 21.205 \\
		\hline
	  \end{tabular}
	}
\end{table}

Por último, antes de realizar el entrenamiento del modelo con la CNN se normalizan los píxeles de las imágenes a valores entre 0 y 1 para ayudar al modelo a converger mejor.

\section{Entrenamiento del modelo}

Tras el preprocesado de los datos, se procede al entrenamiento de un modelo de red neuronal convolucional (CNN) con los datos. La red neuronal creada será una red convolucional con 10 capas en total, que se diferencian en:

\begin{itemize}

    \item \textbf{Capa Conv2D:} son capas convolucionales que aplican un determinado número de filtros a la imagen, y cada filtro extrae diferentes características de la imagen de entrada, como bordes, colores o formas, cada vez más complejas según la profundidad de la red. Las capas también tienen un tamaño de kernel, el tamaño de la ventana que desliza sobre las imágenes para crear el mapa de características de la imagen. En este caso se hace uso de filtros de 32, 64 y 128 y kernels de 3x3, ya que es un tamaño estándar para capturar suficientes relaciones sin que sea demasiado costoso computacionalmente. La función de activación usada en estas capas convolucionales es la  Unidades Lineales Rectificadas (ReLU) para introducir no linealidad en el modelo y poder aprender patrones más complejos. 
    \item \textbf{Capa MaxPooling2D:} son capas que reducen la dimensionalidad de la imagen (ancho y alto) para la siguiente capa convolucional, para ayudar al modelo a detectar características sin ser afectadas por pequeños cambios de escala y orientación.
    \item \textbf{Capa Flatten:} son capas que aplanan la salida de las capas convolucionales a vectores de una sola dimensión, necesario para la entrada de datos de las capas densas.
	\item \textbf{Capa densa:} son capas completamente conectadas que clasifican la imagen basado en las características extraídas por las capas convolucionales. También se usa Dropout, una técnica que sirve para eliminar unidades de salida de la capa de manera aleatoria para prevenir sobreajuste del modelo. En este caso se usa un Dropout de 0.5, eliminando la mitad de las salidas.
	\item \textbf{Capa de salida:} la última capa de la red, en este caso una capa densa utilizando una función de activación Softmax, que sirve para sumar 1 a la salida y poder interpretarse como valores de probabilidad. 

\end{itemize}

Tras definir el modelo de CNN, es necesario compilarlo para configurar diferentes opciones de entrenamiento, como son el optimizador, la función de pérdida y las métricas

\begin{enumerate}

    \item \textbf{Optimizador:} son algoritmos o métodos responsables de cambiar los atributos de las redes neuronales, como los pesos o la tasa de aprendizaje, con el fin de minimizar la función de pérdida. Por lo tanto, ayudan a minimizar o maximizar una función objetivo (función de pérdida) actualizando de manera iterativa los parámetros del modelo.
		\begin{itemize}
			\item Descenso de Gradiente Estocástico (SDG): es simple y básico que actualiza los pesos de manera iterativa, pero depende mucho de la tasa de aprendizaje. 
			\item Algoritmo de Gradiente Adaptativo (AdaGrad): método que mantiene la tasa de aprendizaje en base a la frecuencia de actualización de cada parámetro mejorando el rendimiento en problemas con gradientes dispersos. La tasa de aprendizaje no requiere ajuste manual, pero puede volverse muy pequeña y detener el aprendizaje.
			\item Propagación de Promedio Cuadrático (RMSprop): método que también mantiene tasas de aprendizaje adaptativas para cada parámetro basado en el promedio de las magnitudes más recientes. Establece el entrenamiento con tasas de aprendizaje más consistentes, pero requiere un buen ajuste de hiperparametros.
			\item Estimación de Momento Adaptativo (Adam): método que combina las ventajas de RMSProp y AdaGrad por lo que mantiene tasas de aprendizaje adaptativos para cada parámetro, pero puede ser computacionalmente más costoso que SDG. Aun así este es el optimizador que se usará al aportar más ventajas que las demás.
		\end{itemize}
    \item \textbf{Función de pérdida:} es la función objetivo que el modelo intenta minimizar durante el entrenamiento, y significa qué tan bien el modelo preside la variable objetivo. Depende del tipo de datos que se entrenan y la tarea, si es clasificación, regresión, etc. Funciones de pérdida comunes son “categorical crossentropy” para tareas de clasificación multiclase mutuamente exclusivas, “binary crossentropy” para tareas de clasificación binaria, “mean square error (MSE)” para tareas de regresión, etc. En este caso se usará “categorical crossentropy” al ser un problema de clasificación con múltiples clases mutuamente excluyentes.
    \item \textbf{Métricas:} valores para evaluar el rendimiento del modelo, parecido a las funciones de pérdida, pero los resultados de la evaluación no se utilizan al entrenar el modelo. Un ejemplo es la precisión (accuracy), el valor que mide la frecuencia que las predicciones son iguales a los valores etiquetados, utilizada en tareas de clasificación, como en este caso.

\end{enumerate}

\section{Analisis de resultados}

El entrenamiento de este modelo resulta una precisión de un 70.14\%. Las siguientes gráficas muestran los valores de la función de pérdida y precisión a lo largo de las rondas de entrenamiento.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{graficasEntrenamiento.png}
	\caption{Gráficas de valores de función de pérdida y valores de precisión.}
	\label{fig:imagen31}
\end{figure}

De la primera gráfica (izquierda), la gráfica de valores de la función de pérdida, se puede apreciar que la pérdida del entrenamiento (azul) disminuye de manera constante, lo que significa que el modelo está aprendiendo correctamente y mejorando en cada iteración. En cambio, la pérdida de la validación (naranja) disminuye en un inicio pero comienza a estabilizarse y aumentar ligeramente después de aproximadamente 15 iteraciones. Esto podría indicar un sobreajuste del modelo, significando que el modelo aprende ruido y detalles no aplicables a los datos generales.

De la segunda gráfica (derecha), la gráfica de valores de precisión, se puede apreciar que la precisión del entrenamiento mejora de manera consistente en el tiempo, lo cual es un resultado esperado ya que el modelo se ajusta cada vez más a los datos de entrenamiento. Pero la precisión con los datos de validación, al igual que la función de pérdida, aumenta en un inicio pero posteriormente se estabiliza, lo que también puede indicar indicios de sobreajuste.

La siguiente imagen de la matriz de confusión muestra la cantidad de valores clasificados correctamente por el modelo cada emoción en comparación con los valores etiquetados reales.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{matrizConfusion.png}
	\caption{Matriz de confusión de predicciones del modelo.}
	\label{fig:imagen32}
\end{figure}

Las entradas que aparecen en la diagonal, verdaderos positivos, representan el número de predicciones correctas para cada tipo de emoción. Se puede apreciar que la mayoría de los valores se predicen de manera correcta, de esto el valor de acierto de un 70.14%.

\begin{itemize}

    \item Neutral: 296 predicciones correctas.
    \item Happiness: 315 predicciones correctas. 
    \item Surprise: 377 predicciones correctas. 
    \item Sadness: 264 predicciones correctas.
    \item Anger: 223 predicciones correctas. 

\end{itemize}

En cambio, las entradas que no están en la diagonal, falsas clasificaciones, representan predicciones incorrectas del modelo en función de cada valor. Las predicciones incorrectas entre las clases “neutral” y “sadness” muestran un número significativo de valores, siendo clasificadas 105 de “sadness” como “neutral” y 85 de “neutral” como “sadness”. Esto puede ser debido a características similares en las imágenes de ambas emociones que el modelo capta y no diferencia de una manera muy exacta.

En general, el modelo con el dataset FER+ ha obtenido un resultado de predicción no excelentemente alto pero razonablemente bueno, siendo un 70.14\% teniendo en cuenta que las imágenes de este dataset eran de una baja calidad y resolución, de 48x48 píxeles y en escala de grises, lo que hace el problema más difícil en comparación con datos de imágenes de alta resolución. Considerar una mejora en la calidad de imagen o uso de técnicas de preprocesamiento que resalten características importantes podría incrementar y mejorar la precisión de este modelo.



\chapter{Marco regulador}

En cuanto al marco regulador de este proyecto, en el proceso de desarrollo del sistema de reconocimiento de emociones en los vehículos, hay que tener en cuenta varias normativas y leyes para que se garantice la legalidad y seguridad de dichos datos personales. Una de las normativas más importantes y relevantes para este proyecto es la Ley Orgánica 3/2018 de Protección de Datos Personales y Garantía de los Derechos Digitales (LOPDGDD) 
\cite{lopd-gdd}.

La Ley Orgánica 3/2018 es una adaptación y complementación al Reglamento General de Protección de Datos (RGPD) de la Unión Europea \cite{rgpd}, en la que se establecen normas sobre la recopilación, el almacenamiento y el uso de los datos personales. En relación a este proyecto, implica que cualquier dato obtenido a través del sistema de reconocimiento de emociones debe tener el nivel de seguridad y privacidad más alto. Esto es debido a que los datos faciales de las personas son datos muy sensibles, y se necesita el consentimiento de los usuarios para su captación, procesamiento y uso. Además, también se tienen que implementar medidas de seguridad para proteger dichos datos de accesos no autorizados y garantizar que no sean usados para otros fines distintos a los de este proyecto. 

Otra normativa relacionada es el Real Decreto 662/2012 \cite{rd-sit}, que establece normas para la implantación de los sistemas inteligentes de transporte (SIT) y promueve la integración de estas tecnologías para mejorar la seguridad y la eficiencia. En relación a nuestro proyecto, la normativa establece que nuestro diseño del sistema de reconocimiento de emociones debe alinearse con los principios de SIT, y asegurar que su implementación no solo mejora la experiencia de usuario, sino que también contribuye a la seguridad en las vías, incluyendo que el sistema debe ser funcional y operable con otros SIT, y cumplir con los estándares definidos en el Real Decreto. 

También existe una Ley de IA de la UE \cite{normativaIA}, todavía en desarrollo, siendo la primera normativa específica sobre el uso de tecnologías de Inteligencia Artificial en Europa. Esta ley introduce requisitos de transparencia, aplicabilidad y responsabilidad para los sistemas IA, que influye en nuestro proyecto como la transparencia en el funcionamiento del sistema, permitiendo a los usuarios entender la manera a la que se procesan sus datos y cómo se toman las decisiones basadas en el análisis de emociones. Además, el sistema debería incluir mecanismos de responsabilidad permitiendo identificar y corregir cualquier error de su funcionamiento, asegurando que el uso del mismo sea de manera ética y justa. 

Tras este análisis del marco regulatorio, se sacan conclusiones acerca del cumplimiento normativo de nuestro proyecto. Se deberá de desarrollar políticas claras de privacidad e información sobre el uso de datos para los usuarios del sistema, además de medidas de seguridad ya sea cifrado de datos o restricción de acceso a los mismos. También se deberá realizar auditorías para asegurar el cumplimiento de las normativas actuales, y adaptarse a cualquier posible cambio.



\chapter{Entorno socio-económico}

\section{Gestión del proyecto}

\subsection{Planificación}

El siguiente diagrama de Gantt detalla la planificación de este proyecto, resultando un total de 72 días, aproximadamente 3.5 meses. 

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{ganttChart.png}
	\caption{Diagrama de Gantt de la planificación del proyecto.}
	\label{fig:imagen33}
\end{figure}

\subsection{Estimación}

\subsubsection{Costes personal}

Para este proyecto se necesitarán los siguientes roles de personal:

\begin{itemize}

    \item \textbf{Arquitecto de software:} responsable de tomar decisiones de alto nivel en la arquitectura del software, y guiar su diseño y desarrollo.
    \item \textbf{Ingeniero de ML:} responsable de desarrollar y entrenar el modelo de red neuronal para la detección de emociones, y optimizar su desempeño.
    \item \textbf{Científico de datos:} responsable de analizar los datos y mejorar los algoritmos utilizados, trabajar en el preprocesamiento de los datos antes del entrenamiento.
	\item \textbf{Ingeniero de ciberseguridad:} responsable de desarrollar estrategias para proteger los sistemas, redes y datos contra posibles amenazas o ataques.
	\item \textbf{Ingeniero de sistemas:} responsable de diseñar, implementar y mantener la infraestructura software.
    \item \textbf{QA Tester:} responsable de realizar pruebas para asegurar la calidad y funcionalidad del sistema, identificando y reportando posibles errores.
    \item \textbf{Project Manager:} responsable de planificación, ejecución y control del proyecto, asegurando que se cumplan los objetivos del presupuesto y tiempo establecidos.

\end{itemize}

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Tabla de costes de personal}}
	{
	  \begin{tabular}{|P{3cm}|P{3cm}|P{3cm}|P{3cm}|}
		\hline
		{\cellcolor{gray}\textbf{Posición}} & {\cellcolor{gray}\textbf{Cantidad}} & {\cellcolor{gray}\textbf{Salario (€/año)}} & {\cellcolor{gray}\textbf{Subtotal (€/3.5 meses)}} \\
		\hline
		Arquitecto de software & 1 & 38.000 & 11.083 \\
		\hline
		Ingeniero de ML & 2 & 40.000 & 11.666 * 2 = 23.332 \\
		\hline
		Científico de datos & 2 & 40.000 & 11.666 * 2 = 23.332 \\
		\hline
		Ingeniero de ciberseguridad & 2 & 35.000 & 10.208 * 2 = 20.416 \\
		\hline
		Ingeniero de sistemas & 2 & 28.000 & 8.166 * 2 = 16.332 \\
		\hline
		QA Tester & 2 & 27.000 & 7.875 * 2 = 15.750 \\
		\hline
		Project Manager & 1 & 48.000 & 14.000 \\
		\hline
		\multicolumn{3}{c|}{\cellcolor{gray}\textbf{Coste total personal}} & 124.245 \\
		\hline
	  \end{tabular}
	}
\end{table}

Los datos de salarios se han obtenido de la guía salarial Talentfy de perfiles IT de España del primer semestre de 2024. 
\cite{salarioTalentfy}

\subsubsection{Costes hardware}

El coste final de los recursos hardware se calcula en base al coste total inicial del producto dividiendo por el tiempo de vida de ese producto y posteriormente multiplicando por la duración del proyecto, resultando el gasto de amortización del recurso hardware, lo que es el coste final. 

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Tabla de costes hardware}}
	{
	  \begin{tabular}{|P{3cm}|P{3cm}|P{3cm}|P{3cm}|}
		\hline
		{\cellcolor{gray}\textbf{Recurso}} & {\cellcolor{gray}\textbf{Cantidad}} & {\cellcolor{gray}\textbf{Coste inicial (€/unidad)}} & {\cellcolor{gray}\textbf{Coste final (€)}} \\
		\hline
		Portátiles de empresa & 12 & 1.329 & 96,9 * 12 = 1.162,8 \\
		\hline
		Cámaras & 4 & 59,49 & 8,68 * 4 = 34,72 \\
		\hline
		Servidor de entrenamiento & 1 & 3.577 & 260,82 \\
		\hline
		Almacenamiento & 4 & 19,99 & 2,92 * 4 = 11,68 \\
		\hline
		Otros accesorios y perifericos & 1 & 1.000 & 145,83 \\
		\hline
		\multicolumn{3}{c|}{\cellcolor{gray}\textbf{Coste total hardware}} & 1.615,85 \\
		\hline
	  \end{tabular}
	}
\end{table}

El tiempo de vida de los PCs son de aproximadamente 4 años, y de las cámaras y los accesorios 2 años. Por lo que el coste final se calcula multiplicando por su tiempo de amortización, 3.5 meses/48 meses y 3.5 meses/ 24 meses respectivamente.

\subsubsection{Costes software}

El coste software dependería solo de las posibles licencias de software que se puedan a llegar a usar en el desarrollo del proyecto. En este caso se podría considerar la licencia para el IDE Pycharm de Jetbrains con un coste anual de 249€ por cada empleado, y dando un coste inicial de 249 * 12 = 2.988€ y un coste de amortización de 2.988 / 12 * 3,5 = 871,5€.

También se podría considerar la licencia de StarUML para el arquitecto de software, con un coste inicial anual de 122 € y un coste de amortización de 122 / 12 * 3,5 = 35,58€.

Por lo tanto el coste total de software final sería de 907,08€.

\subsubsection{Resumen de costes}

En la siguiente tabla se muestra el coste total de cada parte, además del coste total final. Tener en cuenta que en la tabla también se muestran costes aproximados de materiales fungibles que puedan ser utilizados como por ejemplo hojas de impresora, bolígrafos, carpetas, etc.

\begin{table}[H]
	\ttabbox[\FBwidth]
	{\caption{Tabla de resumen de costes}}
	{
	  \begin{tabular}{|P{9cm}|P{4cm}|}
		\hline
		{\cellcolor{gray}\textbf{Descripción}} & {\cellcolor{gray}\textbf{Total}} \\
		\hline
		Salario de empleados & 124.245€ \\
		\hline
		Equipamiento hardware y software & 1.615,85€ \\
		\hline
		Fungibles & 135€ \\
		\hline
		Total (sin IVA) & 125.995,85€ \\
		\hline
		Beneficios (10\%) & 12.599,56€ \\
		\hline
		Margen de riesgos (15\%) & 18.899,38€ \\
		\hline
		Total (sin IVA) con beneficios y riesgos +15\% & 157.494,79€ \\
		\hline
		Total (sin IVA) con beneficios y riesgos -15\% & 119.696,03€ \\
		\hline
		VAT(21\%) con beneficios y riesgos +15\% & 57.958,07€ \\
		\hline
		VAT(21\%) con beneficios y riesgos -15\% & 20.159,31€ \\
		\hline
		Total (con IVA) con beneficios y riesgos +15\% & 183.953,92€ \\
		\hline
		Total (con IVA) con beneficios y riesgos -15\% & 146.155,16€ \\
		\hline
	  \end{tabular}
	}
\end{table}

\section{Impacto socio-económico}

La implementación de este sistema IA en el ámbito económico puede promover la competitividad entre las diferentes empresas, ya que con este sistema podrían ofrecer una experiencia de usuario mejor, y por lo tanto tener posibilidad de aventajarse y atraer consumidores que busquen una mejor experiencia de usuario. Además, al igual que la creación de cualquier sistema, aumentaría la demanda de elementos hardware y software, asi como los servicios de mantenimiento de estos sistemas.

Pero no todo es positivo, el uso de este sistema también podría tener efectos económicos negativos, ya sean los costes de desarrollo e implementación iniciales como el de mantenimiento posterior, podría ser difícil de lograr para empresas pequeñas o startup.

En relación al impacto social, es también significativo, ya que mejora la seguridad y el confort de los pasajeros. Desde el punto de vista de la experiencia de usuario, puede reducir el estrés de los pasajeros y aumentar su satisfacción durante un viaje. Desde el punto de vista de seguridad, es posible usarse para detectar emociones que puedan afectar a la seguridad de los pasajeros, y por lo tanto a otros usuarios de las vías, o ayudar a actuar de manera autónoma al vehículo.

Pero por otro lado, también tiene que asegurarse que tanto la privacidad como el uso ético de los datos captados sea gestionado de manera segura y solo para los fines que se especifican en este proyecto. Esta transparencia en el uso de los datos y en el funcionamiento del sistema es la parte esencial para obtener confianza y que el público lo acepte.

En relación al impacto medioambiental, puede ser positivo si se utiliza para optimizar el rendimiento y reducir el consumo de combustible, por ejemplo ajustando el sistema de calefacción de manera más eficiente. Pero puede ser todo lo contrario, que aumente el gasto al usar más calefacción de lo normal, siendo en este caso implementado en vehículos no eléctricos. Si se implementara en vehículos eléctricos no habría este problema, y también sería un beneficio medioambiental si esto fomenta a más personas a comprar vehículos eléctricos.

Y en cuanto al impacto ético, como se ha mencionado anteriormente, la principal preocupación es la privacidad y el uso de los datos generados. En este caso es fundamental que se implementen políticas estrictas de protección de datos y que esos datos sólo sean usados para usos transparentes mencionados en este proyecto.



\chapter{Conclusiones y trabajos futuros}

El desarrollo e implementación de un sistema de reconocimiento de emociones faciales en vehículos ha demostrado ser una herramienta valiosa tanto para mejorar la experiencia del usuario como para aumentar la seguridad en la conducción. A través del análisis y procesamiento de datos en tiempo real, el sistema podría ajustar dinámicamente diversos aspectos del ambiente del vehículo, como la música, la iluminación y la temperatura, ofreciendo una experiencia más personalizada y confortable para los pasajeros.

En términos generales, los objetivos iniciales planteados han sido cumplidos satisfactoriamente. Se ha desarrollado un modelo de reconocimiento de emociones faciales que ha alcanzado una precisión razonable del 70.14\% utilizando el dataset FER+. También se ha diseñado e implementado un sistema que integra el modelo con los sistemas del vehículo, permitiendo la interacción con controles como la iluminación y el audio. Además, se ha realizado un análisis de viabilidad económica y técnica, concluyendo que la integración de esta tecnología es factible y presenta un impacto positivo en la experiencia del usuario y en la seguridad de los pasajeros. No obstante, también es importante considerar los posibles costes iniciales de desarrollo e implementación, que pueden ser una barrera para empresas más pequeñas. Además, la privacidad y seguridad de los datos son aspectos críticos que deben ser gestionados con políticas estrictas y transparentes para asegurar la confianza del usuario.

La realización de este proyecto ha sido una experiencia enriquecedora tanto a nivel profesional como personal. La aplicación de conocimientos adquiridos a lo largo de la carrera en Ingeniería Informática ha sido fundamental para abordar los diversos desafíos técnicos y metodológicos que surgieron. Asignaturas clave como Inteligencia Artificial, Aprendizaje Automático me han aportado los conocimientos esenciales para la realización de este proyecto, y la asignatura de Dirección de Proyectos de Desarrollo Software ha sido fundamental para la planificación y organización de este proyecto.

El sistema desarrollado es solo una simple idea de diseño, que se podría mejorar de muchas maneras y realizar futuros trabajos en relación:

\begin{enumerate}

    \item \textbf{Mejora de la Calidad de Datos:} implementar técnicas avanzadas de preprocesamiento de imágenes y utilizar datasets de mayor resolución para mejorar la precisión del modelo. Incluir clasificaciones con más emociones y características más claras para una mejor precisión en la detección.
    \item \textbf{Integración con ADAS:} desarrollar funcionalidades adicionales que integren el reconocimiento de emociones con los sistemas avanzados de ayuda a la conducción para una experiencia de usuario más completa y segura, o incluir en sistemas ya existentes para diferentes propósitos.
    \item \textbf{Evaluación en Entornos Reales:} realizar pruebas exhaustivas en escenarios de uso real para validar el desempeño del sistema en condiciones diversas y dinámicas.
    \item \textbf{Optimización y Eficiencia:} investigar métodos para optimizar el modelo y reducir el coste computacional, facilitando su implementación en tiempo real y en dispositivos con recursos limitados.

\end{enumerate}

En conclusión, el presente trabajo no solo abre nuevas posibilidades para la personalización y seguridad en los vehículos, sino que también sienta las bases para futuras innovaciones en la integración de inteligencia artificial y aprendizaje automático en la industria automotriz.



%----------
%	BIBLIOGRAFÍA
%----------	

%\nocite{*} % Si quieres que aparezcan en la bibliografía todos los documentos que la componen (también los que no estén citados en el texto) descomenta está línea

\clearpage
\addcontentsline{toc}{chapter}{Bibliografía}
\setquotestyle[english]{british} % Cambiamos el tipo de cita porque en el estilo IEEE se usan las comillas inglesas.
\printbibliography



%----------
%	ANEXOS
%----------	

% Si tu trabajo incluye anexos, puedes descomentar las siguientes líneas
%\chapter* {Anexo x}
%\pagenumbering{gobble} % Las páginas de los anexos no se numeran



\end{document}